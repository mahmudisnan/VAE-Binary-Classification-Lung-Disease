{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b18acb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2202341086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2307410255</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2302394915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2408458946</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2306409371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>2402436028</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>2402436030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>2402436038</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>2402436122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>2402436133</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>796 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  class_id\n",
       "0    2202341086         1\n",
       "1    2307410255         1\n",
       "2    2302394915         1\n",
       "3    2408458946         1\n",
       "4    2306409371         1\n",
       "..          ...       ...\n",
       "791  2402436028         0\n",
       "792  2402436030         0\n",
       "793  2402436038         0\n",
       "794  2402436122         0\n",
       "795  2402436133         1\n",
       "\n",
       "[796 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = \"Chest/ChestNew/trainfiltered.csv\"\n",
    "test_path = \"Chest/ChestNew/testfiltered.csv\"\n",
    "image_folder = \"Chest/ChestNew/\"\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "train_df = train_df[[\"Name\", \"class_id\"]]\n",
    "test_df = test_df[[\"Name\", \"class_id\"]]\n",
    "\n",
    "\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69a6bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_folder, transform_aug=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.image_folder = image_folder\n",
    "        self.transform_aug = transform_aug\n",
    "        self.raw_tensor = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.dataframe.loc[idx, \"Name\"]\n",
    "        label = int(self.dataframe.loc[idx, \"class_id\"])\n",
    "\n",
    "        # Full path ke gambar\n",
    "        img_path = os.path.join(self.image_folder, str(img_name) + \".png\")\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "\n",
    "        x_input, x_raw = self.transform_aug(image)\n",
    "\n",
    "        return x_input, x_raw, torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f00d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BDSRC\\miniconda3\\envs\\ViTlung\\Lib\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.6' (you have '2.0.5'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class CombinedAugmentation:\n",
    "    def __init__(self):\n",
    "        self.albumentations_transform = A.Compose([\n",
    "            A.Resize(224, 224),\\\n",
    "            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=10, p=0.8),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.5),\n",
    "            A.GaussNoise(var_limit=(5.0, 20.0), p=0.5),\n",
    "            A.MotionBlur(p=0.2)\n",
    "            \n",
    "        ])\n",
    "\n",
    "        self.norm = A.Normalize(mean=(0.5,), \n",
    "                                std=(0.5,))\n",
    "        \n",
    "        self.to_tensor = ToTensorV2()\n",
    "\n",
    "        self.torch_transform = T.Compose([\n",
    "            # Augmentasi tambahan setelah tensor (opsional)\n",
    "            T.RandomErasing(p=0.3, scale=(0.02, 0.2))\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img_pil):\n",
    "        img = np.array(img_pil)\n",
    "\n",
    "        augmented = self.albumentations_transform(image=img)['image']\n",
    "\n",
    "        img_raw = T.ToTensor()(Image.fromarray(augmented))\n",
    "\n",
    "        normed = self.norm(image=augmented)['image']\n",
    "\n",
    "        tensor_normed = self.to_tensor(image=normed)['image']\n",
    "        \n",
    "        return tensor_normed, img_raw\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d35d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\BDSRC\\miniconda3\\envs\\ViTlung\\Lib\\site-packages\\albumentations\\core\\validation.py:87: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\961393361.py:18: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(5.0, 20.0), p=0.5),\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi transform gabungan\n",
    "train_transform = CombinedAugmentation()\n",
    "\n",
    "# Dataset & Dataloader\n",
    "train_dataset = ChestXrayDataset(train_df, image_folder, transform_aug=train_transform)\n",
    "test_dataset = ChestXrayDataset(test_df, image_folder, transform_aug=train_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7983a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class VAE_Classifier(nn.Module):\n",
    "    def __init__(self, latent_dim=64, num_classes=2, input_size=(3, 224, 224)):\n",
    "        super(VAE_Classifier, self).__init__()\n",
    "        resnet = models.resnet101(weights=models.ResNet101_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        self.encoder_local = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1\n",
    "        )\n",
    "\n",
    "        self.encoder_global = nn.Sequential(\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "        self.pool_local = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.pool_global = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Hitung flatten_dim secara dinamis\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_size)\n",
    "            local_out = self.encoder_local(dummy_input)\n",
    "            global_out = self.encoder_global(local_out)\n",
    "\n",
    "            pooled_local = self.pool_local(local_out)\n",
    "            pooled_global = self.pool_global(global_out)\n",
    "\n",
    "            flat_local = torch.flatten(pooled_local, start_dim=1)\n",
    "            flat_global = torch.flatten(pooled_global, start_dim=1)\n",
    "\n",
    "            self.flatten_dim = flat_local.shape[1] + flat_global.shape[1] \n",
    "            self.decoder_input_shape = pooled_global.shape[1:]  # (C, H, W)\n",
    "\n",
    "        print(f\"flatten_dim: {self.flatten_dim}\")\n",
    "\n",
    "        # Encoder output ke latent space\n",
    "        self.fc_mu = nn.Linear(self.flatten_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flatten_dim, latent_dim)\n",
    "\n",
    "        # Decoder dari latent ke feature map\n",
    "        self.decoder_fc = nn.Linear(latent_dim, 512 * 7 * 7)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (512, 7, 7)),\n",
    "            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "        # Classifier dari latent space\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar, deterministic=False):\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        else:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "\n",
    "    def forward(self, x, deterministic=False):\n",
    "        x_local = self.encoder_local(x)\n",
    "        x_global = self.encoder_global(x_local)\n",
    "\n",
    "        pooled_local = self.pool_local(x_local)\n",
    "        pooled_global = self.pool_global(x_global)\n",
    "\n",
    "        flat_local = torch.flatten(pooled_local, start_dim=1)\n",
    "        flat_global = torch.flatten(pooled_global, start_dim=1)\n",
    "\n",
    "        x_flat = torch.cat([flat_local, flat_global], dim=1)\n",
    "\n",
    "        mu = self.fc_mu(x_flat)\n",
    "        logvar = self.fc_logvar(x_flat)\n",
    "        logvar = torch.clamp(logvar, min=-10, max=10)\n",
    "\n",
    "        z = self.reparameterize(mu, logvar, deterministic=deterministic)\n",
    "        \n",
    "        x_recon = self.decoder_fc(z)\n",
    "        x_recon = self.decoder(x_recon)\n",
    "        y_pred = self.classifier(z)\n",
    "        \n",
    "        return x_recon, mu, logvar, y_pred\n",
    "\n",
    "def loss_function(x_recon, x, mu, logvar, y_pred, y_true, alpha=1.0, beta=0.0001, gamma=2.0):\n",
    "    recon_loss = nn.functional.mse_loss(x_recon, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) \n",
    "    class_loss = nn.functional.cross_entropy(y_pred, y_true)\n",
    "    \n",
    "    # Debug print (aktifkan sementara)\n",
    "    print(f\"recon: {recon_loss.item():.4f}, kl: {kl_loss.item():.4f}, cls: {class_loss.item():.4f}\")\n",
    "    \n",
    "    return alpha * recon_loss + beta * kl_loss + gamma * class_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1957819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahmudisnan18\u001b[0m (\u001b[33mmahmudisnan18-binus-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\BDSRC\\Documents\\Mahmud\\wandb\\run-20250511_184442-5qcvdea7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung/runs/5qcvdea7' target=\"_blank\">vae_resnet101_run1</a></strong> to <a href='https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung' target=\"_blank\">https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung/runs/5qcvdea7' target=\"_blank\">https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung/runs/5qcvdea7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:20: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to C:\\Users\\BDSRC/.cache\\torch\\hub\\checkpoints\\resnet101-63fe2227.pth\n",
      "100%|██████████| 171M/171M [00:15<00:00, 11.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_dim: 2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:31<00:00,  3.66s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:21<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "Train Loss: 1.4332, Accuracy: 0.5352\n",
      "Val   Loss: 1.3592, Accuracy: 0.6950, F1: 0.6928\n",
      "✅ Best model saved at epoch 1 with F1: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:27<00:00,  3.49s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:21<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/200\n",
      "Train Loss: 1.3677, Accuracy: 0.6332\n",
      "Val   Loss: 1.3470, Accuracy: 0.6550, F1: 0.6414\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:27<00:00,  3.48s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/200\n",
      "Train Loss: 1.3618, Accuracy: 0.6319\n",
      "Val   Loss: 1.3011, Accuracy: 0.7250, F1: 0.7182\n",
      "✅ Best model saved at epoch 3 with F1: 0.7182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:27<00:00,  3.49s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/200\n",
      "Train Loss: 1.3158, Accuracy: 0.6884\n",
      "Val   Loss: 1.2884, Accuracy: 0.7150, F1: 0.7141\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:29<00:00,  3.58s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/200\n",
      "Train Loss: 1.2964, Accuracy: 0.7010\n",
      "Val   Loss: 1.2813, Accuracy: 0.7250, F1: 0.7191\n",
      "✅ Best model saved at epoch 5 with F1: 0.7191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:26<00:00,  3.46s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/200\n",
      "Train Loss: 1.2997, Accuracy: 0.6922\n",
      "Val   Loss: 1.2829, Accuracy: 0.7350, F1: 0.7350\n",
      "✅ Best model saved at epoch 6 with F1: 0.7350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:25<00:00,  3.43s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/200\n",
      "Train Loss: 1.2899, Accuracy: 0.7198\n",
      "Val   Loss: 1.2545, Accuracy: 0.7300, F1: 0.7299\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:27<00:00,  3.48s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/200\n",
      "Train Loss: 1.2707, Accuracy: 0.7261\n",
      "Val   Loss: 1.2328, Accuracy: 0.7550, F1: 0.7511\n",
      "✅ Best model saved at epoch 8 with F1: 0.7511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:26<00:00,  3.44s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/200\n",
      "Train Loss: 1.2594, Accuracy: 0.7462\n",
      "Val   Loss: 1.2244, Accuracy: 0.7300, F1: 0.7273\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:26<00:00,  3.46s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/200\n",
      "Train Loss: 1.2537, Accuracy: 0.7462\n",
      "Val   Loss: 1.2282, Accuracy: 0.7500, F1: 0.7416\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:26<00:00,  3.45s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/200\n",
      "Train Loss: 1.2322, Accuracy: 0.7399\n",
      "Val   Loss: 1.1952, Accuracy: 0.7450, F1: 0.7448\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:25<00:00,  3.44s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/200\n",
      "Train Loss: 1.2332, Accuracy: 0.7538\n",
      "Val   Loss: 1.2220, Accuracy: 0.7600, F1: 0.7580\n",
      "✅ Best model saved at epoch 12 with F1: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:26<00:00,  3.47s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:20<00:00,  2.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/200\n",
      "Train Loss: 1.2191, Accuracy: 0.7739\n",
      "Val   Loss: 1.2185, Accuracy: 0.7450, F1: 0.7395\n",
      "No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:24<00:00,  3.38s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:19<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/200\n",
      "Train Loss: 1.1946, Accuracy: 0.7839\n",
      "Val   Loss: 1.2141, Accuracy: 0.7550, F1: 0.7532\n",
      "No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:21<00:00,  3.25s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:19<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/200\n",
      "Train Loss: 1.1976, Accuracy: 0.7827\n",
      "Val   Loss: 1.1806, Accuracy: 0.7550, F1: 0.7540\n",
      "No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:21<00:00,  3.27s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:19<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/200\n",
      "Train Loss: 1.1951, Accuracy: 0.7726\n",
      "Val   Loss: 1.1962, Accuracy: 0.7500, F1: 0.7406\n",
      "No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:43: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
      "  torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
      "Training: 100%|██████████| 25/25 [01:21<00:00,  3.28s/it]\n",
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:19<00:00,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/200\n",
      "Train Loss: 1.1720, Accuracy: 0.7902\n",
      "Val   Loss: 1.1772, Accuracy: 0.7600, F1: 0.7565\n",
      "No improvement for 5 epoch(s).\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>train/accuracy</td><td>▁▄▄▅▆▅▆▆▇▇▇▇█████</td></tr><tr><td>train/loss</td><td>█▆▆▅▄▄▄▄▃▃▃▃▂▂▂▂▁</td></tr><tr><td>val/accuracy</td><td>▄▁▆▅▆▆▆█▆▇▇█▇██▇█</td></tr><tr><td>val/f1</td><td>▄▁▆▅▆▇▆█▆▇▇█▇██▇█</td></tr><tr><td>val/loss</td><td>██▆▅▅▅▄▃▃▃▂▃▃▂▁▂▁</td></tr><tr><td>val/precision</td><td>▃▁▅▃▅▅▄▇▅█▅▇▇▇▆█▇</td></tr><tr><td>val/recall</td><td>▄▁▆▅▆▆▆█▆▇▇█▇██▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_epoch</td><td>12</td></tr><tr><td>best_val_acc</td><td>0.76</td></tr><tr><td>best_val_f1</td><td>0.75804</td></tr><tr><td>best_val_loss</td><td>1.22199</td></tr><tr><td>epoch</td><td>17</td></tr><tr><td>train/accuracy</td><td>0.7902</td></tr><tr><td>train/loss</td><td>1.17195</td></tr><tr><td>val/accuracy</td><td>0.76</td></tr><tr><td>val/f1</td><td>0.75649</td></tr><tr><td>val/loss</td><td>1.17717</td></tr><tr><td>val/precision</td><td>0.77331</td></tr><tr><td>val/recall</td><td>0.75888</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vae_resnet101_run1</strong> at: <a href='https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung/runs/5qcvdea7' target=\"_blank\">https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung/runs/5qcvdea7</a><br> View project at: <a href='https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung' target=\"_blank\">https://wandb.ai/mahmudisnan18-binus-university/vae-classifier-lung</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250511_184442-5qcvdea7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import wandb\n",
    "import torch\n",
    "\n",
    "wandb.init(\n",
    "    project=\"vae-classifier-lung\",\n",
    "    name=\"vae_resnet101_run1\",\n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"epochs\": 200,\n",
    "        \"latent_dim\": 512,\n",
    "        \"batch_size\": 32,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"model\": \"VAE_Classifier_resnet101\"\n",
    "    }\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VAE_Classifier(latent_dim=512, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "def train(model, loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for x_input, x_raw, labels in tqdm(loader, desc=\"Training\"):\n",
    "        x_input, x_raw, labels = x_input.to(device), x_raw.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            x_recon, mu, logvar, y_pred = model(x_input)\n",
    "            recon_loss = nn.functional.binary_cross_entropy_with_logits(x_recon, x_raw, reduction='mean')\n",
    "            kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) / x_input.size(0)\n",
    "            class_loss = nn.functional.cross_entropy(y_pred, labels)\n",
    "            loss = recon_loss + 0.0001 * kl_loss + class_loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        running_loss += loss.item() * x_input.size(0)\n",
    "        preds = torch.argmax(y_pred, dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# --- Validation Loop ---\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_input, x_raw, labels in tqdm(loader, desc=\"Validation\"):\n",
    "            x_input, x_raw, labels = x_input.to(device), x_raw.to(device), labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                x_recon, mu, logvar, y_pred = model(x_input)\n",
    "                recon_loss = nn.functional.binary_cross_entropy_with_logits(x_recon, x_raw, reduction='mean')\n",
    "                kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp()) / x_input.size(0)\n",
    "                class_loss = nn.functional.cross_entropy(y_pred, labels)\n",
    "                loss = recon_loss + 0.0001 * kl_loss + class_loss\n",
    "\n",
    "            running_loss += loss.item() * x_input.size(0)\n",
    "            preds = torch.argmax(y_pred, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    accuracy = correct / total\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, all_labels, all_preds\n",
    "\n",
    "# --- Training Configuration ---\n",
    "best_val_f1 = 0.0\n",
    "patience = 5\n",
    "counter = 0\n",
    "early_stop = False\n",
    "num_epochs = wandb.config.epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer)\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1, val_labels, val_preds = validate(model, test_loader)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/accuracy\": train_acc,\n",
    "        \"val/loss\": val_loss,\n",
    "        \"val/accuracy\": val_acc,\n",
    "        \"val/precision\": val_prec,\n",
    "        \"val/recall\": val_rec,\n",
    "        \"val/f1\": val_f1\n",
    "    })\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        best_epoch = epoch + 1\n",
    "        counter = 0\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': best_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_accuracy': val_acc,\n",
    "            'val_precision': val_prec,\n",
    "            'val_recall': val_rec,\n",
    "            'val_f1': val_f1\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, \"best_model_resnet101_checkpoint.pt\")\n",
    "        print(f\"✅ Best model saved at epoch {best_epoch} with F1: {val_f1:.4f}\")\n",
    "\n",
    "        wandb.summary[\"best_epoch\"] = best_epoch\n",
    "        wandb.summary[\"best_val_f1\"] = best_val_f1\n",
    "        wandb.summary[\"best_val_acc\"] = val_acc\n",
    "        wandb.summary[\"best_val_loss\"] = val_loss\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "        print(f\"No improvement for {counter} epoch(s).\")\n",
    "        if counter >= patience:\n",
    "            early_stop = True\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a2aa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_dim: 2304\n",
      "===> Best Model Info:\n",
      "Epoch        : 12\n",
      "Val Accuracy : 0.7600\n",
      "Val Precision: 0.7669\n",
      "Val Recall   : 0.7592\n",
      "Val F1-score : 0.7580\n",
      "Val Loss     : 1.2220\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VAE_Classifier(latent_dim=512, num_classes=2)\n",
    "checkpoint = torch.load(\"best_model_resnet101_checkpoint.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"===> Best Model Info:\")\n",
    "print(f\"Epoch        : {checkpoint['epoch']}\")\n",
    "print(f\"Val Accuracy : {checkpoint['val_accuracy']:.4f}\")\n",
    "print(f\"Val Precision: {checkpoint['val_precision']:.4f}\")\n",
    "print(f\"Val Recall   : {checkpoint['val_recall']:.4f}\")\n",
    "print(f\"Val F1-score : {checkpoint['val_f1']:.4f}\")\n",
    "print(f\"Val Loss     : {checkpoint['val_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c583bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_dim: 2304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/7 [00:00<?, ?it/s]C:\\Users\\BDSRC\\AppData\\Local\\Temp\\ipykernel_15312\\860792306.py:67: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Validation: 100%|██████████| 7/7 [00:19<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===> Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class_0       0.72      0.88      0.79       101\n",
      "     class_1       0.84      0.66      0.74        99\n",
      "\n",
      "    accuracy                           0.77       200\n",
      "   macro avg       0.78      0.77      0.77       200\n",
      "weighted avg       0.78      0.77      0.77       200\n",
      "\n",
      "Accuracy Score: 0.77\n",
      "Val F1-score  : 0.7666396103896104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Pastikan model dan checkpoint sudah diload:\n",
    "model = VAE_Classifier(latent_dim=512, num_classes=2)\n",
    "checkpoint = torch.load(\"best_model_resnet101_checkpoint.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluasi ulang di test_loader (atau val_loader)\n",
    "val_loss, val_acc, val_prec, val_rec, val_f1, val_labels, val_preds = validate(model, test_loader)\n",
    "\n",
    "# Konversi ke list int\n",
    "val_labels = np.array(val_labels).astype(int).tolist()\n",
    "val_preds = np.array(val_preds).astype(int).tolist()\n",
    "\n",
    "# Cetak classification report\n",
    "print(\"\\n===> Classification Report:\")\n",
    "print(classification_report(val_labels, val_preds, target_names=[\"class_0\", \"class_1\"], zero_division=0))\n",
    "\n",
    "print(\"Accuracy Score:\", accuracy_score(val_labels, val_preds))\n",
    "print(f\"Val F1-score  :\", val_f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ViTlung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
